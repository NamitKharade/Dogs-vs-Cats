{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cats Vs Dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Compressed', 'test', 'train']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "print(os.listdir(\"../Data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import cv2                 \n",
    "import numpy as np         \n",
    "import os                  \n",
    "from random import shuffle\n",
    "from tqdm import tqdm      \n",
    "\n",
    "train_dir = '../Data/train'\n",
    "test_dir = '../Data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "c80cd80be9d850003f68e59057d5ba9a6d52b43a"
   },
   "outputs": [],
   "source": [
    "def get_label(img):\n",
    "    label = img.split('.')[0]\n",
    "    if label == 'cat': \n",
    "        return [1,0]\n",
    "    elif label == 'dog': \n",
    "        return [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Y and test_y as one hot array.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing and collecting train and test data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "f0f1ada45edbc50ee8a715361b535e76dc8731ec"
   },
   "outputs": [],
   "source": [
    "def making_train_data():\n",
    "    training_data = []\n",
    "    \n",
    "    for img in tqdm(os.listdir(train_dir)):\n",
    "        label = get_label(img)\n",
    "        path = os.path.join(train_dir,img)\n",
    "        img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (50,50))\n",
    "        training_data.append([np.array(img),np.array(label)])\n",
    "        \n",
    "    shuffle(training_data)\n",
    "    np.save('train_data.npy', training_data)\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "e75ed4e473a4291280a90dfcd7e5bfb21457e954"
   },
   "outputs": [],
   "source": [
    "def making_test_data():\n",
    "    testing_data = []\n",
    "    \n",
    "    for img in tqdm(os.listdir(test_dir)):\n",
    "        path = os.path.join(test_dir , img)\n",
    "        img_num = img.split('.')[0]\n",
    "        img = cv2.imread(path , cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img , (50,50))\n",
    "        testing_data.append([np.array(img), img_num])\n",
    "        \n",
    "    shuffle(testing_data)\n",
    "    np.save('test_data.npy', testing_data)\n",
    "    return testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "2c3359f6cfe6e81a34e28831a9c6f48e5dbc5588"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25000/25000 [01:36<00:00, 258.63it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = making_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "b9f6580cdf4598e86c4db3fe262aff82aa7c6120",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "57e5d2067ad790344118caa9c3a480499a1fb513"
   },
   "outputs": [],
   "source": [
    "train = train_data[0:20000]\n",
    "test = train_data[20000:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "c77070b911daae6f0be5d8556270966c7f639978"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 5000\n"
     ]
    }
   ],
   "source": [
    "print(len(train) , len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "8cf6fd88eb44e07ddc163cbd9bd8b8433619d371"
   },
   "outputs": [],
   "source": [
    "X = np.array([i[0] for i in train]).reshape(-1,1,50,50)\n",
    "Y = [i[1] for i in train]\n",
    "\n",
    "test_x = np.array([i[0] for i in test]).reshape(-1,1,50,50)\n",
    "test_y = [i[1] for i in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ImageDataGenerator to avoid overfitting the model with training data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "49511c921e399fd0d7a8c53133d5421ea45a2eb6"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  \n",
    "        samplewise_center=False,  \n",
    "        featurewise_std_normalization=False,  \n",
    "        samplewise_std_normalization=False,  \n",
    "        zca_whitening=False,  \n",
    "        rotation_range=10,  \n",
    "        zoom_range = 0.0,  \n",
    "        width_shift_range=0.1,  \n",
    "        height_shift_range=0.1,  \n",
    "        horizontal_flip=False, \n",
    "        vertical_flip=False)  \n",
    "\n",
    "datagen.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using ReduceLROnPlateau to reduce the learning rate after certain epochs with callbacks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "88c97ab677d33df3c5da5c58509688778fd9dbef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python 3.7\\lib\\site-packages\\keras\\callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_acc', factor=0.1, epsilon=0.0001, patience=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "283a16994d0b7f9d2415faf3c3128a73afd86f1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       ...,\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.asarray(Y)\n",
    "Y.reshape(len(Y) , 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "719b0f7eea83f3911e12505913568b302f878cc2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       ...,\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y = np.asarray(test_y)\n",
    "test_y.reshape(len(test_y) , 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "9295d203164fc199cd4784db0d49c175450508a7"
   },
   "outputs": [],
   "source": [
    "test_x = test_x.reshape(-1, 1, 50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "577c6251872bde2b13b54be8edea2095e1fd0405"
   },
   "outputs": [],
   "source": [
    "test_x = test_x / 255\n",
    "X = X / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "8c6a06cbab7c8fcd7ec4aec37dbe0eb5a9ab2c6f"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense , Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Conv2D , BatchNormalization\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "da9272b73522ea4388780ba10b622aa11d7ee3f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\python 3.7\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\python 3.7\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\python 3.7\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\python 3.7\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\python 3.7\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\python 3.7\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\python 3.7\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\python 3.7\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\python 3.7\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\python 3.7\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\python 3.7\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\python 3.7\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From d:\\python 3.7\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\python 3.7\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\python 3.7\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 50, 50)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 50, 50)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 25, 25)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 25, 25)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 64, 25, 25)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 64, 12, 12)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 96, 12, 12)        55392     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 96, 10, 10)        83040     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 96, 5, 5)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 128, 5, 5)         110720    \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 128, 3, 3)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 128, 1, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 470,114\n",
      "Trainable params: 470,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def swish_activation(x):\n",
    "    return (K.sigmoid(x) * x)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding=\"same\", input_shape=(1,50,50)))\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(96, (3, 3), dilation_rate=(2, 2), activation='relu', padding=\"same\"))\n",
    "model.add(Conv2D(96, (3, 3), padding=\"valid\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), dilation_rate=(2, 2), activation='relu', padding=\"same\"))\n",
    "model.add(Conv2D(128, (3, 3), padding=\"valid\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation=swish_activation))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(2 , activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "671e43a7e0f1866e6dc751157f6d39aa96755d35"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam' , metrics=['accuracy'])\n",
    "steps_per_epoch = len(train_data) // batch_size\n",
    "validation_steps = len((test_x, test_y)) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "9ca97682b7bedf82496cb760f10b8872ae6dd567"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\python 3.7\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\python 3.7\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch 1/20\n",
      " - 39s - loss: 0.6883 - acc: 0.5305 - val_loss: 0.6770 - val_acc: 0.5727\n",
      "Epoch 2/20\n",
      " - 19s - loss: 0.6665 - acc: 0.6002 - val_loss: 0.6469 - val_acc: 0.6215\n",
      "Epoch 3/20\n",
      " - 18s - loss: 0.6209 - acc: 0.6592 - val_loss: 0.5619 - val_acc: 0.7131\n",
      "Epoch 4/20\n",
      " - 18s - loss: 0.5667 - acc: 0.7089 - val_loss: 0.5298 - val_acc: 0.7356\n",
      "Epoch 5/20\n",
      " - 18s - loss: 0.5060 - acc: 0.7546 - val_loss: 0.4520 - val_acc: 0.7830\n",
      "Epoch 6/20\n",
      " - 19s - loss: 0.4609 - acc: 0.7814 - val_loss: 0.4184 - val_acc: 0.8069\n",
      "Epoch 7/20\n",
      " - 19s - loss: 0.4252 - acc: 0.8032 - val_loss: 0.3887 - val_acc: 0.8245\n",
      "Epoch 8/20\n",
      " - 19s - loss: 0.3851 - acc: 0.8236 - val_loss: 0.3808 - val_acc: 0.8282\n",
      "Epoch 9/20\n",
      " - 18s - loss: 0.3679 - acc: 0.8337 - val_loss: 0.3622 - val_acc: 0.8367\n",
      "Epoch 10/20\n",
      " - 19s - loss: 0.3504 - acc: 0.8412 - val_loss: 0.3471 - val_acc: 0.8415\n",
      "Epoch 11/20\n",
      " - 18s - loss: 0.3317 - acc: 0.8543 - val_loss: 0.3450 - val_acc: 0.8505\n",
      "Epoch 12/20\n",
      " - 19s - loss: 0.3116 - acc: 0.8626 - val_loss: 0.3586 - val_acc: 0.8454\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/20\n",
      " - 19s - loss: 0.2744 - acc: 0.8813 - val_loss: 0.3360 - val_acc: 0.8552\n",
      "Epoch 14/20\n",
      " - 18s - loss: 0.2523 - acc: 0.8895 - val_loss: 0.3280 - val_acc: 0.8642\n",
      "Epoch 15/20\n",
      " - 19s - loss: 0.2523 - acc: 0.8916 - val_loss: 0.3338 - val_acc: 0.8586\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 16/20\n",
      " - 18s - loss: 0.2448 - acc: 0.8965 - val_loss: 0.3372 - val_acc: 0.8582\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 17/20\n",
      " - 18s - loss: 0.2406 - acc: 0.8973 - val_loss: 0.3340 - val_acc: 0.8597\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 18/20\n",
      " - 19s - loss: 0.2445 - acc: 0.8949 - val_loss: 0.3337 - val_acc: 0.8599\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "Epoch 19/20\n",
      " - 18s - loss: 0.2447 - acc: 0.8967 - val_loss: 0.3337 - val_acc: 0.8600\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "Epoch 20/20\n",
      " - 18s - loss: 0.2437 - acc: 0.8957 - val_loss: 0.3337 - val_acc: 0.8600\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(datagen.flow(X, Y, batch_size=batch_size),\n",
    "                    steps_per_epoch=X.shape[0] // batch_size,\n",
    "                    callbacks=[lr_reduce],\n",
    "                    validation_data=(test_x, test_y),\n",
    "                    epochs = epochs, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaining a validation accuracy of 87.15 % with a self network architecture laid down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG-16 gave around 88.3 % as validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f41c9194c281f9e2432b7ad32a9ca37f8f2d0291"
   },
   "source": [
    "score = model.evaluate(test_x, test_y, verbose=0)\n",
    "print('valid loss:', score[0])\n",
    "print('valid accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "2dd16d81767f4493072e8bb13eeda453c1a119da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Graphs are laid separately.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [00:54<00:00, 227.91it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data = making_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('submission_file.csv','w') as f:\n",
    "#     f.write('id,label\\n')\n",
    "            \n",
    "# with open('submission_file.csv','a') as f:\n",
    "#     for data in tqdm(test_data):\n",
    "#         img_num = data[1]\n",
    "#         img_data = data[0]\n",
    "#         orig = img_data\n",
    "#         data = img_data.reshape(1,1,50,50)\n",
    "#         model_out = model.predict([data])[0]\n",
    "#         f.write('{},{}\\n'.format(img_num,model_out[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**'submission_file' as the final submission file for the predictions in the test_data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../Model/Cats_and_Dogs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think the animal is :  Cat\n"
     ]
    }
   ],
   "source": [
    "new_image_1 = cv2.imread('../cat.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "new_image_1 = cv2.resize(new_image_1 , (50,50))\n",
    "\n",
    "cv2.imshow(\"Animal Picture\", new_image_1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "resized_data = new_image_1.reshape(1,1,50,50)\n",
    "iso_result = model.predict([resized_data])\n",
    "\n",
    "# print(iso_result[0][0])\n",
    "# print(iso_result[0][1])\n",
    "\n",
    "result_animal = \"Cat\" if iso_result[0][0] > iso_result[0][1] else \"Dog\"\n",
    "print(\"I think the animal is : \", result_animal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think the animal is :  Cat\n"
     ]
    }
   ],
   "source": [
    "# Short-Hand version of above cell...\n",
    "new_image_2 = cv2.resize(cv2.imread('../cat.jpg', cv2.IMREAD_GRAYSCALE), (50, 50)).reshape(1, 1, 50, 50)\n",
    "\n",
    "result_animal = \"Cat\" if iso_result[0][0] > iso_result[0][1] else \"Dog\"\n",
    "print(\"I think the animal is : \", result_animal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think the animal is :  Cat\n"
     ]
    }
   ],
   "source": [
    "# trying to load model from disk...\n",
    "from keras.models import load_model\n",
    "\n",
    "model_2 = load_model(\"../Model/Cats_and_Dogs.h5\", custom_objects = {\"swish_activation\": swish_activation})   \n",
    "iso_result = model_2.predict([resized_data])\n",
    "new_image_2 = cv2.resize(cv2.imread('../cat.jpg', cv2.IMREAD_GRAYSCALE), (50, 50)).reshape(1, 1, 50, 50) \n",
    "result_animal = \"Cat\" if iso_result[0][0] > iso_result[0][1] else \"Dog\"\n",
    "print(\"I think the animal is : \", result_animal)\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
